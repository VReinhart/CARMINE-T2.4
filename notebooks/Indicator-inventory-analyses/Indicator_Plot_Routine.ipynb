{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc92b09c",
   "metadata": {},
   "source": [
    "# CARMINE — Indicator Plot Routine (Single Maps + Panel Maps)\n",
    "\n",
    "This notebook consolidates the **single-map** routine (part 1) and the **panel-map** routine (part 2) into one workflow.\n",
    "\n",
    "**Including**\n",
    "- path imports for working, data & outputs\n",
    "- User-driven configuration (edit variables in the *User input* cell).\n",
    "- Helpers for reducing indicator NetCDFs to plottable 2D maps\n",
    "- Saving plots for the use in stakeholder communications or scientific presentations\n",
    "\n",
    "Works for *all* indicators by discovering indicator NetCDFs in the CSA `INDICATORS/` folders.\n",
    "\n",
    "If you start Jupyter from `notebooks/`, imports work out of the box through the path helper cell (copied from `notebooks/_lib/paths.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports (Path helper cell) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Bootstrap: add (one level up) so we can import _lib from repo root\n",
    "REPO_ROOT_BOOT = Path.cwd().resolve().parent  # assumes we are in .../notebooks\n",
    "if str(REPO_ROOT_BOOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT_BOOT))\n",
    "\n",
    "from _lib.paths import (\n",
    "    REPO_ROOT as REPO_ROOT_HELPER,\n",
    "    iter_indicators_dirs,\n",
    "    iter_indicator_files,\n",
    ")\n",
    "\n",
    "# Optional: Cartopy (maps)\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "except Exception:\n",
    "    ccrs = None\n",
    "\n",
    "# ✅ From here on: use the helper's repo root (should be CARMINE-T2.4/)\n",
    "REPO_ROOT = REPO_ROOT_HELPER\n",
    "\n",
    "WORK      = REPO_ROOT / \"notebooks\"\n",
    "OUTPUTS   = REPO_ROOT / \"outputs\"\n",
    "FIG_DIR   = OUTPUTS / \"figures\"\n",
    "TABLE_DIR = OUTPUTS / \"tables\"\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"WORK:\", WORK)\n",
    "print(\"REPO_ROOT_HELPER:\", REPO_ROOT_HELPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER INPUT (edit here) ---\n",
    "\n",
    "# Target selection\n",
    "CSA_SELECT       = \"Leipzig\"      # e.g., \"Barcelona\", \"Birmingham\", \"Bologna\", \"Funen-Odense\", \"Leipzig\"\n",
    "DATASET_FILTER   = \"eobs\"           # e.g., \"eobs\", \"cerra\", \"eu-cordex-11\" ; \"era5\" \"emo\" (Birmingham) not working currently euro cordex nw for Funen-Odense\n",
    "INDICATOR_SELECT = \"HD\"           # e.g., \"HD\", \"WSDI\", \"TXX\", \"RX5DAY\", \"CDD\", \"SU\"\n",
    "PERIOD_SELECT    = \"1991-2020\"    # e.g., \"1981-2010\", \"1991-2020\", \"2036-2065\"\n",
    "\n",
    "# Plot styling\n",
    "CMAP_SINGLE      = \"Blues\"\n",
    "CMAP_PANEL       = \"Blues\"\n",
    "ROBUST_PCTS      = (2, 98)        # for shared panel color scale (vmin/vmax from percentiles)\n",
    "\n",
    "# Boundary overlay (optional)\n",
    "ADD_FUA_BOUNDARY = True\n",
    "FUA_SHP_REL      = Path(\"shapefile/UI-boundaries-FUA/FUA_Boundaries.shp\")  # relative to repo root\n",
    "FUA_NAME_FIELD   = \"FUA_NAME\"     # field name used in the FUA boundary shapefile\n",
    "FUA_MAPPING = {                  # mapping from CSA folder name -> FUA_NAME (only if needed)\n",
    "    \"Prague\": \"Praha\",\n",
    "    \"Funen-Odense\": \"Odense\",\n",
    "    \"Athens\": \"Athina\",\n",
    "    \"Birmingham\": \"West Midlands urban area\",\n",
    "}\n",
    "\n",
    "# Output\n",
    "SINGLE_OUT_SUBDIR = Path(\"single_maps\")\n",
    "PANEL_OUT_SUBDIR  = Path(\"panels\")\n",
    "DPI               = 200\n",
    "SHOW_PLOTS        = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca9692-81f4-4843-b892-f2577a0b4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing boundary shapefiles.\n",
    "import geopandas as gpd\n",
    "\n",
    "fua_path = REPO_ROOT / FUA_SHP_REL\n",
    "gdf = gpd.read_file(fua_path)\n",
    "\n",
    "print(\"Columns:\", list(gdf.columns))\n",
    "print(\"CRS:\", gdf.crs)\n",
    "print(\"Count:\", len(gdf))\n",
    "\n",
    "# show unique names (first 50)\n",
    "names = sorted(gdf[FUA_NAME_FIELD].dropna().astype(str).unique())\n",
    "print(\"Unique FUA names (sample):\", names[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8cb8b-2e87-4c44-aae5-d967cb921265",
   "metadata": {},
   "source": [
    "## Helpers (discovery + plotting)\n",
    " \n",
    "The helpers below implement the workflow in a reusable and consistent way for both Part 1 (single maps) and Part 2 (panel plots) of the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "##### 1) Filename and period parsing utilities\n",
    "\n",
    "These helpers support robust file matching and period handling.\n",
    "\n",
    "- **`normalize_token()`**  \n",
    "  Normalizes strings for robust matching (case-insensitive, ignores hyphens and underscores).\n",
    "\n",
    "- **`parse_years_from_string()`**  \n",
    "  Extracts all `YYYY–YYYY` year pairs found in a filename or string.  \n",
    "  Used to interpret period information encoded in indicator filenames.\n",
    "---\n",
    "\n",
    "##### 2) NetCDF → 2D map reduction pipeline\n",
    "\n",
    "These helpers are used together to transform an indicator NetCDF into a single 2D\n",
    "`xarray.DataArray` that can be plotted.\n",
    "\n",
    "- **`ensure_lonlat_coords()`**  \n",
    "  Attaches geographic longitude/latitude coordinates if they exist in the dataset\n",
    "  as variables or coordinates (common for CERRA / CORDEX data with `y/x` dimensions).\n",
    "\n",
    "- **`time_subset_for_period()`**  \n",
    "  Subsets the time dimension to a given `YYYY–YYYY` period, if a time axis is present.\n",
    "\n",
    "- **`to_numeric_days_if_timedelta()`**  \n",
    "  Converts `timedelta64` data to numeric days and ensures a floating-point dtype\n",
    "  for robust plotting.\n",
    "\n",
    "- **`reduce_to_map()`**  \n",
    "  The main reduction helper:\n",
    "  1. Selects the most appropriate variable for the given indicator.\n",
    "  2. Attaches lon/lat coordinates if needed.\n",
    "  3. Applies optional temporal subsetting.\n",
    "  4. Aggregates over time (mean) to produce a single 2D field.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3) Coordinate detection for plotting\n",
    "\n",
    "- **`get_lon_lat_names()`**  \n",
    "  Identifies valid geographic longitude/latitude coordinate names in a DataArray or Dataset.  \n",
    "  This function intentionally **does not** fall back to `x/y`, which are usually projected.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4) Optional boundary overlay (FUA)\n",
    "\n",
    "- **`load_fua_boundary()`**  \n",
    "  Loads a CSA-specific Functional Urban Area (FUA) boundary polygon if enabled and available.\n",
    "  Used only when plotting with boundary overlays.\n",
    "\n",
    "---\n",
    "\n",
    "##### 5) Indicator file discovery\n",
    "\n",
    "- **`discover_indicator_files()`**  \n",
    "  Scans the repository for NetCDF indicator files under  \n",
    "  `<CSA>/INDICATORS/**/*.nc` and returns a DataFrame inventory\n",
    "  (`csa`, `path`, `name`).\n",
    "\n",
    "---\n",
    "\n",
    "##### 6) Plotting\n",
    "\n",
    "- **`plot_map_2d()`**  \n",
    "  Plots a single 2D DataArray and saves it to disk.  \n",
    "  Uses Cartopy when geographic lon/lat coordinates are available; otherwise falls back\n",
    "  to a simple `imshow` plot.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes for maintainers**\n",
    "- Most users do **not** need to modify these helpers; configurable behavior is handled\n",
    "  in the dedicated user input cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7228cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers (discovery + plotting) ---\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
    "\n",
    "# only used by reduce_to_map\n",
    "def ensure_lonlat_coords(ds: xr.Dataset, da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    If ds contains lon/lat (or longitude/latitude) as variables, attach them\n",
    "    as coords to da (common for CERRA/CORDEX where da dims are y/x).\n",
    "    \"\"\"\n",
    "    # preferred names first\n",
    "    candidates = [\n",
    "        (\"lon\", \"lat\"),\n",
    "        (\"longitude\", \"latitude\"),\n",
    "        (\"LON\", \"LAT\"),\n",
    "        (\"nav_lon\", \"nav_lat\"),\n",
    "    ]\n",
    "    for lon_name, lat_name in candidates:\n",
    "        if lon_name in ds and lat_name in ds:\n",
    "            try:\n",
    "                return da.assign_coords({lon_name: ds[lon_name], lat_name: ds[lat_name]})\n",
    "            except Exception:\n",
    "                pass\n",
    "        if lon_name in ds.coords and lat_name in ds.coords:\n",
    "            try:\n",
    "                return da.assign_coords({lon_name: ds.coords[lon_name], lat_name: ds.coords[lat_name]})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return da\n",
    "\n",
    "    if period_label:\n",
    "        yrs = []\n",
    "        for m in re.finditer(r\"(19\\d{2}|20\\d{2})\\D{0,3}(19\\d{2}|20\\d{2})\", name):\n",
    "            y1, y2 = int(m.group(1)), int(m.group(2))\n",
    "    if 1900 <= y1 <= 2100 and 1900 <= y2 <= 2100 and y2 >= y1:\n",
    "        yrs.append((y1, y2))\n",
    "\n",
    "        try:\n",
    "            y1, y2 = map(int, period_label.split(\"-\"))\n",
    "        except Exception:\n",
    "            return True\n",
    "        if yrs:\n",
    "            return any((a==y1 and b==y2) for a,b in yrs) or any((a<=y1 and b>=y2) for a,b in yrs)\n",
    "        # If no years in name, we can't reject it; time axis may cover it\n",
    "        return True\n",
    "    return True\n",
    "\n",
    "# only used by reduce_to_map\n",
    "def time_subset_for_period(da: xr.DataArray, period_label: str) -> xr.DataArray:\n",
    "    \"\"\"If time axis exists, subset to YYYY-YYYY (inclusive).\"\"\"\n",
    "    if \"time\" not in getattr(da, \"dims\", []):\n",
    "        return da\n",
    "    try:\n",
    "        y1, y2 = map(int, period_label.split(\"-\"))\n",
    "        years = pd.DatetimeIndex(da[\"time\"].values).year\n",
    "        return da.sel(time=(years >= y1) & (years <= y2))\n",
    "    except Exception:\n",
    "        return da\n",
    "# only used by reduce_to_map\n",
    "def to_numeric_days_if_timedelta(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"timedelta64 → float days; else force float64 for robust plotting.\"\"\"\n",
    "    if np.issubdtype(da.dtype, np.timedelta64):\n",
    "        out = (da / np.timedelta64(1, \"D\")).astype(\"float64\")\n",
    "        units = str(da.attrs.get(\"units\", \"\")).strip()\n",
    "        out.attrs[\"units\"] = \"days\" if units == \"\" else f\"{units} (days)\"\n",
    "        return out\n",
    "    return da.astype(\"float64\", copy=False)\n",
    "\n",
    "def reduce_to_map(ds: xr.Dataset, indicator_token: str, period_label: str | None) -> xr.DataArray:\n",
    "    # --- pick variable (inlined choose_var) ---\n",
    "    candidates = list(ds.data_vars)\n",
    "    if not candidates:\n",
    "        raise ValueError(\"No data variables found in dataset.\")\n",
    "    hits = [v for v in candidates if str(indicator_token).lower() in v.lower()]\n",
    "    var = hits[0] if hits else candidates[0]\n",
    "\n",
    "    # --- reduce to 2D map ---\n",
    "    da = ensure_lonlat_coords(ds, ds[var].squeeze())\n",
    "\n",
    "    if period_label:\n",
    "        da = time_subset_for_period(da, period_label)\n",
    "\n",
    "    if \"time\" in getattr(da, \"dims\", []):\n",
    "        with xr.set_options(keep_attrs=True):\n",
    "            da = da.mean(\"time\", skipna=True)\n",
    "\n",
    "    da = to_numeric_days_if_timedelta(da)\n",
    "    return da\n",
    "\n",
    "def parse_years_from_string(s: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Return all (YYYY,YYYY) pairs found in a string.\"\"\"\n",
    "    s = str(s)\n",
    "    hits: list[tuple[int, int]] = []\n",
    "    for m in re.finditer(r\"(19\\d{2}|20\\d{2})\\D{0,3}(19\\d{2}|20\\d{2})\", s):\n",
    "        y1, y2 = int(m.group(1)), int(m.group(2))\n",
    "        if 1900 <= y1 <= 2100 and 1900 <= y2 <= 2100 and y2 >= y1:\n",
    "            hits.append((y1, y2))\n",
    "    return hits\n",
    "\n",
    "    \n",
    "def get_lon_lat_names(obj) -> tuple[str | None, str | None]:\n",
    "    # Only accept true geographic coordinate names\n",
    "    for lon, lat in [(\"lon\",\"lat\"), (\"longitude\",\"latitude\"), (\"LON\",\"LAT\"), (\"nav_lon\",\"nav_lat\")]:\n",
    "        if lon in obj.coords and lat in obj.coords:\n",
    "            return lon, lat\n",
    "        if lon in getattr(obj, \"dims\", []) and lat in getattr(obj, \"dims\", []):\n",
    "            return lon, lat\n",
    "\n",
    "    # If lon/lat exist as *variables* (not coords), allow them too\n",
    "    for lon, lat in [(\"lon\",\"lat\"), (\"longitude\",\"latitude\"), (\"LON\",\"LAT\"), (\"nav_lon\",\"nav_lat\")]:\n",
    "        if lon in obj and lat in obj:\n",
    "            return lon, lat\n",
    "\n",
    "    # Do NOT fall back to x/y here (those are usually projected)\n",
    "    return None, None\n",
    "\n",
    "def load_fua_boundary(csa_name: str):\n",
    "    if not ADD_FUA_BOUNDARY or gpd is None:\n",
    "        return None\n",
    "    shp = (REPO_ROOT / FUA_SHP_REL)\n",
    "    if not shp.exists():\n",
    "        warnings.warn(f\"FUA shapefile not found at: {shp}\")\n",
    "        return None\n",
    "    gdf = gpd.read_file(shp)\n",
    "    fua_name = FUA_MAPPING.get(csa_name, csa_name)\n",
    "    if FUA_NAME_FIELD in gdf.columns:\n",
    "        sel = gdf[gdf[FUA_NAME_FIELD].astype(str) == str(fua_name)]\n",
    "        if sel.empty:\n",
    "            warnings.warn(f\"No FUA boundary found for '{fua_name}' in {shp.name}\")\n",
    "            return None\n",
    "        return sel\n",
    "    warnings.warn(f\"Field '{FUA_NAME_FIELD}' not in shapefile. Available: {list(gdf.columns)[:10]}...\")\n",
    "    return None\n",
    "\n",
    "def discover_indicator_files(root: Path | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discover NetCDF indicator files under CSA/INDICATORS folders.\n",
    "\n",
    "    Expected layout:\n",
    "        REPO_ROOT/\n",
    "          <CSA_NAME>/\n",
    "            INDICATORS/\n",
    "              *.nc\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "        - csa\n",
    "        - path\n",
    "        - name\n",
    "    \"\"\"\n",
    "    root = Path(root) if root is not None else REPO_ROOT\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for csa_dir in root.iterdir():\n",
    "        if not csa_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        ind_dir = csa_dir / \"INDICATORS\"\n",
    "        if not ind_dir.exists():\n",
    "            continue\n",
    "\n",
    "        for fp in ind_dir.rglob(\"*.nc\"):\n",
    "            rows.append({\n",
    "                \"csa\": csa_dir.name,\n",
    "                \"path\": fp,\n",
    "                \"name\": fp.name,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        warnings.warn(\n",
    "            f\"No indicator NetCDF files found under {root}. \"\n",
    "            \"Check repo layout and INDICATORS folder names.\"\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_map_2d(\n",
    "    da: xr.DataArray,\n",
    "    title: str,\n",
    "    out_png: Path,\n",
    "    cmap: str = \"Blues\",\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    boundary_gdf=None,\n",
    "    dpi: int = 200,\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"Plot a single 2D DataArray map.\n",
    "    Uses Cartopy if available; otherwise falls back to imshow.\n",
    "    \"\"\"\n",
    "    da = da.squeeze()\n",
    "\n",
    "    # Always define these so we never get NameError\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "    ax = None\n",
    "    cbar = None\n",
    "\n",
    "    lon_name, lat_name = get_lon_lat_names(da)\n",
    "    lon = da[lon_name].values if lon_name and lon_name in da.coords else None\n",
    "    lat = da[lat_name].values if lat_name and lat_name in da.coords else None\n",
    "    arr = np.asarray(da.values, dtype=\"float64\")\n",
    "\n",
    "    use_cartopy = (ccrs is not None) and (lon is not None) and (lat is not None)\n",
    "\n",
    "    if use_cartopy:\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        ax.coastlines(resolution=\"10m\", linewidth=0.6)\n",
    "\n",
    "        if getattr(lon, \"ndim\", 0) == 1 and getattr(lat, \"ndim\", 0) == 1:\n",
    "            extent = [\n",
    "                float(np.nanmin(lon)), float(np.nanmax(lon)),\n",
    "                float(np.nanmin(lat)), float(np.nanmax(lat)),\n",
    "            ]\n",
    "            im = ax.imshow(arr, origin=\"lower\", extent=extent, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        else:\n",
    "            im = ax.pcolormesh(lon, lat, arr, cmap=cmap, vmin=vmin, vmax=vmax, shading=\"auto\")\n",
    "\n",
    "        # Boundary overlay + zoom ONLY possible with cartopy axis\n",
    "        if boundary_gdf is not None:\n",
    "            try:\n",
    "                bg = boundary_gdf\n",
    "                if getattr(bg, \"crs\", None) is not None:\n",
    "                    bg = bg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "                bg.boundary.plot(ax=ax, linewidth=1.2, color=\"black\")\n",
    "\n",
    "                minx, miny, maxx, maxy = bg.total_bounds\n",
    "                pad_x = (maxx - minx) * 0.15\n",
    "                pad_y = (maxy - miny) * 0.15\n",
    "                ax.set_extent(\n",
    "                    [minx - pad_x, maxx + pad_x, miny - pad_y, maxy + pad_y],\n",
    "                    crs=ccrs.PlateCarree(),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Failed to plot or zoom boundary: {e}\")\n",
    "\n",
    "        ax.set_title(title)\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.05)\n",
    "\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "        im = ax.imshow(arr, origin=\"lower\", cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(title + \" (no cartopy)\")\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.05)\n",
    "\n",
    "    # Safe colorbar label\n",
    "    units = str(da.attrs.get(\"units\", \"\")).strip()\n",
    "    if units and cbar is not None:\n",
    "        cbar.set_label(units)\n",
    "\n",
    "    out_png = Path(out_png)\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png, dpi=dpi, bbox_inches=\"tight\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\"Saved:\", out_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913e1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PART 1 — SINGLE MAPS (one CSA, one indicator, one period)\n",
    "# ============================================================\n",
    "\n",
    "# --- Ensure file inventory exists ---\n",
    "if \"files_df\" not in globals() or files_df is None or len(files_df) == 0:\n",
    "    print(\"(info) Building files_df via discover_indicator_files() ...\")\n",
    "    files_df = discover_indicator_files()\n",
    "    print(\"(info) Discovered:\", len(files_df), \"files\")\n",
    "else:\n",
    "    print(\"(info) Using existing files_df with\", len(files_df), \"files\")\n",
    "\n",
    "# --- Select candidate files (ROBUST matching like yesterday) ---\n",
    "cand = files_df.copy()\n",
    "\n",
    "# CSA match (case-insensitive)\n",
    "cand = cand[cand[\"csa\"].astype(str).str.strip().str.lower() == str(CSA_SELECT).strip().lower()].copy()\n",
    "\n",
    "# normalize names once to avoid hyphen/underscore issues (eu-cordex-11 vs eu_cordex_11 etc.)\n",
    "cand[\"name_norm\"] = cand[\"name\"].astype(str).apply(normalize_token)\n",
    "\n",
    "# Dataset filter (robust)\n",
    "if DATASET_FILTER:\n",
    "    cand = cand[cand[\"name_norm\"].str.contains(normalize_token(DATASET_FILTER), na=False)]\n",
    "\n",
    "# Indicator filter (robust)\n",
    "cand = cand[cand[\"name_norm\"].str.contains(normalize_token(INDICATOR_SELECT), na=False)]\n",
    "\n",
    "# Prefer period matches (based on years parsed from filename, but tolerant)\n",
    "y1, y2 = map(int, PERIOD_SELECT.split(\"-\"))\n",
    "\n",
    "def _pref_period(name: str) -> int:\n",
    "    yrs = parse_years_from_string(name)\n",
    "    if not yrs:\n",
    "        return 1  # unknown -> keep but lower preference\n",
    "    return 0 if any((a == y1 and b == y2) or (a <= y1 and b >= y2) for a, b in yrs) else 2\n",
    "\n",
    "cand[\"period_pref\"] = cand[\"name\"].apply(_pref_period)\n",
    "cand = cand.sort_values([\"period_pref\", \"name\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"CSA_SELECT:\", CSA_SELECT)\n",
    "print(\"INDICATOR_SELECT:\", INDICATOR_SELECT, \"| PERIOD_SELECT:\", PERIOD_SELECT, \"| DATASET_FILTER:\", DATASET_FILTER)\n",
    "print(\"Candidates:\", len(cand))\n",
    "display(cand[[\"csa\", \"path\", \"name\", \"period_pref\"]].head(20))\n",
    "\n",
    "if cand.empty:\n",
    "    raise FileNotFoundError(\n",
    "        \"No indicator file found for the given selection. \"\n",
    "        \"Adjust CSA/indicator/dataset/period filters.\"\n",
    "    )\n",
    "\n",
    "# --- Pick best candidate ---\n",
    "fp = Path(cand.iloc[0][\"path\"])\n",
    "print(\"Using:\", fp)\n",
    "\n",
    "# Dataset tag for output filename\n",
    "dataset_tag = DATASET_FILTER if DATASET_FILTER else \"mixed\"\n",
    "\n",
    "# --- Load -> reduce to 2D map ---\n",
    "ds = xr.open_dataset(fp, decode_timedelta=True)\n",
    "try:\n",
    "    da_map = reduce_to_map(ds, INDICATOR_SELECT, PERIOD_SELECT)\n",
    "finally:\n",
    "    try:\n",
    "        ds.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "arr = np.asarray(da_map.values)\n",
    "print(\"da_map dims:\", da_map.dims, \"| shape:\", da_map.shape)\n",
    "with np.errstate(all=\"ignore\"):\n",
    "    print(\"da_map min/max:\", float(np.nanmin(arr)), float(np.nanmax(arr)))\n",
    "\n",
    "# --- Boundary + output path ---\n",
    "boundary = load_fua_boundary(CSA_SELECT)  # may be None (that's fine)\n",
    "out_png = FIG_DIR / SINGLE_OUT_SUBDIR / f\"{CSA_SELECT}_{dataset_tag}_{INDICATOR_SELECT}_{PERIOD_SELECT}.png\"\n",
    "\n",
    "# --- Plot + save ---\n",
    "plot_map_2d(\n",
    "    da_map,\n",
    "    title=f\"{CSA_SELECT} — {INDICATOR_SELECT} — {PERIOD_SELECT} ({dataset_tag})\",\n",
    "    out_png=out_png,\n",
    "    cmap=CMAP_SINGLE,\n",
    "    boundary_gdf=boundary,\n",
    "    dpi=DPI,\n",
    "    show=SHOW_PLOTS,\n",
    ")\n",
    "\n",
    "print(\"Done. Saved to:\", out_png, \"| exists:\", out_png.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66384ba4-8f0f-42b5-8fc6-afe0922fe734",
   "metadata": {},
   "source": [
    "> ⚠️ **Work in progress (Part 2 – panel plots)**  \n",
    ">  \n",
    "> The Part 2 workflow is still under development. At the moment, it cannot be fully tested or validated because the currently available indicator NetCDFs differ in spatial extent and preprocessing (e.g. CSA-specific cropping).  \n",
    ">  \n",
    "> As a result, panel plots and cross-dataset comparisons may not yet behave consistently across all case study areas. Once a harmonized and testable set of indicators is available (e.g. consistent spatial units and extents), this part of the notebook will be revisited and finalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PART 2 — PANEL MAPS (all CSAs for one indicator + period)\n",
    "# ============================================================\n",
    "#\n",
    "# Goal: produce a quick, comparable overview across CSAs for the *selected*\n",
    "# dataset + indicator + period, using the same helper functions as Part 1.\n",
    "#\n",
    "# Notes:\n",
    "# - Panel plots use plain imshow (no cartopy), because CSAs may have different\n",
    "#   spatial extents / grids (especially if indicator NetCDFs are pre-cropped).\n",
    "# - A shared color scale is computed across all included CSAs using robust\n",
    "#   percentiles (ROBUST_PCTS).\n",
    "\n",
    "# --- Ensure file inventory exists (after kernel restart) ---\n",
    "if \"files_df\" not in globals() or files_df is None or len(files_df) == 0:\n",
    "    print(\"(info) Building files_df via discover_indicator_files() ...\")\n",
    "    files_df = discover_indicator_files()\n",
    "    print(\"(info) Discovered:\", len(files_df), \"files\")\n",
    "else:\n",
    "    print(\"(info) Using existing files_df with\", len(files_df), \"files\")\n",
    "\n",
    "if len(files_df) == 0:\n",
    "    raise RuntimeError(\"files_df is empty. Fix discover_indicator_files() root=... first.\")\n",
    "\n",
    "# --- Local helpers used by Part 2 (self-contained) ---\n",
    "def _pref_period_for_name(name: str, period_label: str) -> int:\n",
    "    \"\"\"Lower is better. Prefers files that match/cover the requested period in filename.\"\"\"\n",
    "    yrs = parse_years_from_string(name)\n",
    "    if not yrs:\n",
    "        return 1  # unknown -> keep but lower preference\n",
    "    y1, y2 = map(int, period_label.split(\"-\"))\n",
    "    return 0 if any((a == y1 and b == y2) or (a <= y1 and b >= y2) for a, b in yrs) else 2\n",
    "\n",
    "dataset_tag = DATASET_FILTER if DATASET_FILTER else \"mixed\"\n",
    "\n",
    "print(\"Panel selection:\",\n",
    "      \"dataset=\", dataset_tag,\n",
    "      \"| indicator=\", INDICATOR_SELECT,\n",
    "      \"| period=\", PERIOD_SELECT)\n",
    "\n",
    "# --- Build one file per CSA (best matching file) ---\n",
    "panel_rows = []\n",
    "for csa in sorted(files_df[\"csa\"].dropna().unique()):\n",
    "    sub = files_df[files_df[\"csa\"].astype(str) == str(csa)].copy()\n",
    "\n",
    "    if DATASET_FILTER:\n",
    "        sub = sub[sub[\"name\"].str.lower().str.contains(str(DATASET_FILTER).lower(), na=False)]\n",
    "\n",
    "    # coarse file-level indicator filter (variable selection happens in reduce_to_map too)\n",
    "    sub = sub[sub[\"name\"].str.lower().str.contains(str(INDICATOR_SELECT).lower(), na=False)]\n",
    "\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    sub[\"period_pref\"] = sub[\"name\"].apply(lambda n: _pref_period_for_name(n, PERIOD_SELECT))\n",
    "    sub = sub.sort_values([\"period_pref\", \"name\"]).reset_index(drop=True)\n",
    "\n",
    "    panel_rows.append({\"csa\": csa, \"path\": Path(sub.iloc[0][\"path\"]), \"name\": sub.iloc[0][\"name\"]})\n",
    "\n",
    "panel_df = pd.DataFrame(panel_rows)\n",
    "print(\"CSAs with candidate file:\", len(panel_df))\n",
    "display(panel_df.head(20))\n",
    "\n",
    "if panel_df.empty:\n",
    "    print(\"No CSA snapshots available to plot (after filters).\")\n",
    "else:\n",
    "    # --- Load + reduce each CSA to a 2D map ---\n",
    "    snapshots: dict[str, xr.DataArray] = {}\n",
    "    skipped: list[tuple[str, str]] = []\n",
    "\n",
    "    for _, row in panel_df.iterrows():\n",
    "        csa = row[\"csa\"]\n",
    "        fp = Path(row[\"path\"])\n",
    "        try:\n",
    "            ds = xr.open_dataset(fp, decode_timedelta=True)\n",
    "            da_map = reduce_to_map(ds, INDICATOR_SELECT, PERIOD_SELECT)\n",
    "            # Ensure it's 2D-ish for plotting (we use imshow)\n",
    "            da_map = da_map.squeeze()\n",
    "            if len(getattr(da_map, \"shape\", ())) < 2:\n",
    "                skipped.append((csa, f\"not 2D after reduce ({da_map.dims})\"))\n",
    "            else:\n",
    "                snapshots[csa] = da_map\n",
    "        except Exception as e:\n",
    "            skipped.append((csa, f\"{type(e).__name__}: {e}\"))\n",
    "        finally:\n",
    "            try:\n",
    "                ds.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(\"Loaded snapshots:\", len(snapshots))\n",
    "    if skipped:\n",
    "        print(\"Skipped (first 15):\", skipped[:15], f\"… total {len(skipped)}\")\n",
    "\n",
    "    # --- Shared color scale (robust percentiles across all CSAs) ---\n",
    "    if snapshots:\n",
    "        all_vals = []\n",
    "        for da_map in snapshots.values():\n",
    "            v = np.asarray(da_map.values, dtype=\"float64\").ravel()\n",
    "            v = v[np.isfinite(v)]\n",
    "            if v.size:\n",
    "                all_vals.append(v)\n",
    "        if all_vals:\n",
    "            vcat = np.concatenate(all_vals)\n",
    "            p_lo, p_hi = ROBUST_PCTS\n",
    "            vmin = float(np.nanpercentile(vcat, p_lo))\n",
    "            vmax = float(np.nanpercentile(vcat, p_hi))\n",
    "            if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "                vmin, vmax = None, None\n",
    "        else:\n",
    "            vmin, vmax = None, None\n",
    "\n",
    "        print(\"Common scale:\", vmin, \"→\", vmax)\n",
    "\n",
    "        # --- Render panel ---\n",
    "        items = list(snapshots.items())\n",
    "        N = len(items)\n",
    "\n",
    "        ncols = 3 if N >= 9 else 2 if N >= 4 else 1\n",
    "        nrows = int(np.ceil(N / ncols))\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows, ncols,\n",
    "            figsize=(5.2 * ncols, 4.4 * nrows),\n",
    "            constrained_layout=True\n",
    "        )\n",
    "        axes = np.atleast_1d(axes).reshape(nrows, ncols)\n",
    "\n",
    "        mappable = None\n",
    "        for i, (csa, da_map) in enumerate(items):\n",
    "            r, c = divmod(i, ncols)\n",
    "            ax = axes[r, c]\n",
    "\n",
    "            arr = np.asarray(da_map.values, dtype=\"float64\")\n",
    "            im = ax.imshow(arr, origin=\"lower\", cmap=CMAP_PANEL, vmin=vmin, vmax=vmax, aspect=\"auto\")\n",
    "            mappable = im\n",
    "\n",
    "            # Small title with CSA + grid size\n",
    "            ax.set_title(f\"{csa} ({arr.shape[0]}×{arr.shape[1]})\", fontsize=10)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        # Hide unused axes\n",
    "        for j in range(N, nrows * ncols):\n",
    "            r, c = divmod(j, ncols)\n",
    "            axes[r, c].axis(\"off\")\n",
    "\n",
    "        # Colorbar + title + save\n",
    "        if mappable is not None:\n",
    "            cbar = fig.colorbar(mappable, ax=axes.ravel().tolist(), shrink=0.85, pad=0.02)\n",
    "            # Try to label units (first snapshot)\n",
    "            first_units = str(next(iter(snapshots.values())).attrs.get(\"units\", \"\")).strip()\n",
    "            if first_units:\n",
    "                cbar.set_label(first_units)\n",
    "\n",
    "        fig.suptitle(f\"{dataset_tag} — {INDICATOR_SELECT} — {PERIOD_SELECT}\", fontsize=14)\n",
    "\n",
    "        out_png = FIG_DIR / PANEL_OUT_SUBDIR / f\"panel_{dataset_tag}_{INDICATOR_SELECT}_{PERIOD_SELECT}.png\"\n",
    "        out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(out_png, dpi=DPI, bbox_inches=\"tight\")\n",
    "\n",
    "        if SHOW_PLOTS:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(\"Saved panel to:\", out_png, \"| exists:\", out_png.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5885f3-b294-4be7-81bf-748995adbf42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
