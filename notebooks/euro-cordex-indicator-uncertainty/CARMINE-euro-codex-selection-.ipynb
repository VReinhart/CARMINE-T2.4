{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89a7a18-91c6-4094-b7ff-e7266be79fa5",
   "metadata": {},
   "source": [
    "#### Analysis of EURO-CORDEX annual data (in progress)\n",
    "\n",
    "- The CARMINE Euro-Cordex selected experiments have an impact on the climate indicators for the CARMINE CSAs in Europe\n",
    "- This notebook is designed to quantify the uncertainties of the indicators for the CSAs and to provide communication material for stakeholders and scientific conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2214c4-988d-4252-8fb9-2f72fb5c1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "# Try to infer repository root from notebook location\n",
    "# Assumes notebook is somewhere inside the repo\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Heuristic: walk upwards until we find a marker (outputs/ or .git/)\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \".git\").exists() or (p / \"outputs\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "\n",
    "# ---- EURO-CORDEX testing paths ----\n",
    "EUROCORDEX_TESTDATA_ROOT = REPO_ROOT / \"2601_EURO_CORDEX_testing_data\"\n",
    "EUROCORDEX_TEST_UNZIP_ROOT = EUROCORDEX_TESTDATA_ROOT / \"_unzipped\"\n",
    "EUROCORDEX_TEST_OUTPUT_ROOT = REPO_ROOT / \"outputs\" / \"eurocordex_testing\"\n",
    "\n",
    "# Create directories (safe: ignored by git)\n",
    "EUROCORDEX_TEST_UNZIP_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "EUROCORDEX_TEST_OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"EUROCORDEX_TESTDATA_ROOT:\", EUROCORDEX_TESTDATA_ROOT)\n",
    "print(\"EUROCORDEX_TEST_UNZIP_ROOT:\", EUROCORDEX_TEST_UNZIP_ROOT)\n",
    "print(\"EUROCORDEX_TEST_OUTPUT_ROOT:\", EUROCORDEX_TEST_OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9abeaf-9b16-4866-9004-f3c4445c73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INVENTORY — scan unzipped EURO-CORDEX NetCDFs and build `df`\n",
    "# (Run this BEFORE User Inputs + Catalog)\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# --- where the unzipped NetCDFs live ---\n",
    "UNZIP_ROOT = EUROCORDEX_TEST_UNZIP_ROOT\n",
    "print(\"Scanning UNZIP_ROOT:\", UNZIP_ROOT)\n",
    "\n",
    "# --- discover files ---\n",
    "nc_files = sorted([p for p in UNZIP_ROOT.rglob(\"*\") if p.is_file() and p.suffix.lower() in (\".nc\", \".nc4\", \".cdf\")])\n",
    "print(f\"Found {len(nc_files)} NetCDF file(s)\")\n",
    "\n",
    "if not nc_files:\n",
    "    warnings.warn(\"No NetCDFs found. Did the unzip cell run successfully?\")\n",
    "\n",
    "def _first_attr(ds: xr.Dataset, keys: list[str]) -> str | None:\n",
    "    \"\"\"Return the first non-empty global attribute found among keys.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in ds.attrs and ds.attrs.get(k) not in (None, \"\"):\n",
    "            return str(ds.attrs.get(k))\n",
    "    return None\n",
    "\n",
    "rows: list[dict] = []\n",
    "\n",
    "for fp in nc_files:\n",
    "    row = {\n",
    "        # file info\n",
    "        \"file_name\": fp.name,\n",
    "        \"path\": str(fp),\n",
    "        \"zip_folder\": fp.parent.name,  # extracted zip stem folder\n",
    "        \"bytes\": fp.stat().st_size,\n",
    "\n",
    "        # quick structure\n",
    "        \"vars\": None,\n",
    "        \"dims\": None,\n",
    "\n",
    "        # time summary\n",
    "        \"n_time\": None,\n",
    "        \"time_start\": None,\n",
    "        \"time_end\": None,\n",
    "\n",
    "        # origin / provenance (best-effort; may be None depending on provider)\n",
    "        \"institution\": None,\n",
    "        \"source\": None,\n",
    "        \"driving_model\": None,\n",
    "        \"rcm_model\": None,\n",
    "        \"ensemble_member\": None,\n",
    "        \"experiment\": None,\n",
    "        \"domain\": None,\n",
    "        \"grid\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        ds = xr.open_dataset(fp, decode_times=True)\n",
    "        try:\n",
    "            # structure\n",
    "            row[\"vars\"] = \",\".join(list(ds.data_vars.keys()))\n",
    "            row[\"dims\"] = \",\".join([f\"{k}:{v}\" for k, v in ds.sizes.items()])\n",
    "\n",
    "            # time\n",
    "            if \"time\" in ds.sizes:\n",
    "                row[\"n_time\"] = int(ds.sizes[\"time\"])\n",
    "            if \"time\" in ds.coords and ds.sizes.get(\"time\", 0) > 0:\n",
    "                # keep as string (works for cftime calendars too)\n",
    "                row[\"time_start\"] = str(ds[\"time\"].values[0])\n",
    "                row[\"time_end\"] = str(ds[\"time\"].values[-1])\n",
    "\n",
    "            # provenance (common CF / CORDEX attrs)\n",
    "            row[\"institution\"] = _first_attr(ds, [\"institution\", \"institute_id\"])\n",
    "            row[\"source\"] = _first_attr(ds, [\"source\", \"title\"])\n",
    "            row[\"driving_model\"] = _first_attr(ds, [\"driving_model_id\", \"driving_model\"])\n",
    "            row[\"rcm_model\"] = _first_attr(ds, [\"model_id\", \"rcm_model\", \"regional_model_id\"])\n",
    "            row[\"ensemble_member\"] = _first_attr(ds, [\"driving_model_ensemble_member\", \"ensemble_member\", \"realization\"])\n",
    "            row[\"experiment\"] = _first_attr(ds, [\"experiment_id\", \"scenario\", \"scenario_id\"])\n",
    "            row[\"domain\"] = _first_attr(ds, [\"domain_id\", \"CORDEX_domain\"])\n",
    "            row[\"grid\"] = _first_attr(ds, [\"grid\", \"grid_label\"])\n",
    "\n",
    "        finally:\n",
    "            ds.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        # keep the row but mark failure so users see it\n",
    "        row[\"vars\"] = \"ERROR\"\n",
    "        row[\"dims\"] = \"ERROR\"\n",
    "        row[\"source\"] = f\"ERROR: {e}\"\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# --- build df (THIS is what the catalog cell expects) ---\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Show a compact view\n",
    "key_cols = [\n",
    "    \"zip_folder\", \"file_name\",\n",
    "    \"n_time\", \"time_start\", \"time_end\",\n",
    "    \"experiment\", \"driving_model\", \"rcm_model\", \"ensemble_member\",\n",
    "    \"institution\", \"domain\", \"grid\",\n",
    "    \"vars\", \"dims\",\n",
    "]\n",
    "key_cols = [c for c in key_cols if c in df.columns]\n",
    "\n",
    "display(df[key_cols].sort_values([\"zip_folder\", \"file_name\"]).reset_index(drop=True))\n",
    "\n",
    "print(\"\\nInventory dataframe created: df\")\n",
    "print(\"df.shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a91334-50e0-4feb-ac55-d4c93370dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USER INPUTS — EURO-CORDEX test workflow\n",
    "# =============================================================================\n",
    "\n",
    "# --- CSAs (panel order) ---\n",
    "CSA_LIST = [\n",
    "    \"Barcelona\",\n",
    "    \"Bologna\",\n",
    "    \"Birmingham\",\n",
    "    \"Leipzig\",\n",
    "    \"Prague\",\n",
    "    \"Funen-Odense\",\n",
    "    \"Athens\",\n",
    "]\n",
    "\n",
    "# --- Variable / indicator selection ---\n",
    "# For your current test data:\n",
    "VAR_NAME = \"tas\"   # 2m temperature (monthly mean)\n",
    "# Later: \"pr\", \"tasmax\", derived indicators, etc.\n",
    "\n",
    "# --- Model-chain selection ---\n",
    "# Use None to include all available; otherwise filter by substring(s)\n",
    "# (we will match against file_name and/or origin fields from the inventory)\n",
    "MODEL_FILTERS = None\n",
    "# Example:\n",
    "# MODEL_FILTERS = [\"NCC-NorESM1-M\", \"KNMI-RACMO22E\"]\n",
    "\n",
    "# --- Scenario / experiment selection ---\n",
    "EXPERIMENT_FILTERS = None\n",
    "# Example:\n",
    "# EXPERIMENT_FILTERS = [\"rcp26\"]\n",
    "\n",
    "# --- Which products to prepare later (not executed yet) ---\n",
    "DO_MONTHLY_SINGLE = False\n",
    "DO_ANNUAL_MEAN    = True\n",
    "DO_PERIOD_MEAN    = True\n",
    "DO_DIFF_MAPS      = False  # later\n",
    "\n",
    "# Periods for period-mean products (YYYY, YYYY), inclusive\n",
    "PERIODS = [\n",
    "    (2031, 2040),\n",
    "]\n",
    "\n",
    "# --- user / project configuration ---\n",
    "ADD_FUA_BOUNDARY = True\n",
    "\n",
    "FUA_SHP_REL = Path(\"shapefile/UI-boundaries-FUA/FUA_Boundaries.shp\")\n",
    "FUA_NAME_FIELD = \"FUA_NAME\"\n",
    "\n",
    "# Only needed if CSA folder names differ from FUA names\n",
    "FUA_MAPPING = {\n",
    "    \"Prague\": \"Praha\",\n",
    "    \"Funen-Odense\": \"Odense\",\n",
    "    \"Athens\": \"Athina\",\n",
    "    \"Birmingham\": \"West Midlands urban area\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"CSA_LIST:\", CSA_LIST)\n",
    "print(\"VAR_NAME:\", VAR_NAME)\n",
    "print(\"MODEL_FILTERS:\", MODEL_FILTERS)\n",
    "print(\"EXPERIMENT_FILTERS:\", EXPERIMENT_FILTERS)\n",
    "print(\"PERIODS:\", PERIODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d5c99-501f-4b48-b24e-255b60424752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CATALOG — parse inventory into a tidy, filterable table\n",
    "# =============================================================================\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ---- REQUIREMENTS CHECK ----\n",
    "assert \"df\" in globals(), \"Inventory dataframe 'df' not found. Run the inventory cell first.\"\n",
    "\n",
    "required_cols = {\"file_name\", \"path\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Inventory df is missing required columns: {missing}\")\n",
    "\n",
    "cat = df.copy()\n",
    "\n",
    "# ---- Parse variable + date range from filename (robust for EURO-CORDEX naming) ----\n",
    "# Example:\n",
    "# tas_EUR-11_NCC-NorESM1-M_rcp26_r1i1p1_KNMI-RACMO22E_v1_mon_203101-204012.nc\n",
    "rx = re.compile(\n",
    "    r\"^(?P<var>[A-Za-z0-9]+)_\"                 # tas\n",
    "    r\"(?P<domain>EUR-\\d{2})_\"                  # EUR-11\n",
    "    r\"(?P<gcm>[^_]+)_\"                         # NCC-NorESM1-M\n",
    "    r\"(?P<exp>rcp\\d{2})_\"                      # rcp26\n",
    "    r\"(?P<member>r\\d+i\\d+p\\d+)_\"               # r1i1p1\n",
    "    r\"(?P<rcm>[^_]+)_\"                         # KNMI-RACMO22E\n",
    "    r\".*?_mon_\"                                # ..._mon_\n",
    "    r\"(?P<start>\\d{6})-(?P<end>\\d{6})\"         # 203101-204012\n",
    ")\n",
    "\n",
    "def parse_from_filename(name: str) -> dict:\n",
    "    m = rx.search(str(name))\n",
    "    if not m:\n",
    "        return {\n",
    "            \"var_fn\": None, \"domain_fn\": None, \"gcm_fn\": None,\n",
    "            \"exp_fn\": None, \"member_fn\": None, \"rcm_fn\": None,\n",
    "            \"start_ym\": None, \"end_ym\": None\n",
    "        }\n",
    "    d = m.groupdict()\n",
    "    return {\n",
    "        \"var_fn\": d[\"var\"],\n",
    "        \"domain_fn\": d[\"domain\"],\n",
    "        \"gcm_fn\": d[\"gcm\"],\n",
    "        \"exp_fn\": d[\"exp\"],\n",
    "        \"member_fn\": d[\"member\"],\n",
    "        \"rcm_fn\": d[\"rcm\"],\n",
    "        \"start_ym\": d[\"start\"],\n",
    "        \"end_ym\": d[\"end\"],\n",
    "    }\n",
    "\n",
    "parsed = cat[\"file_name\"].apply(parse_from_filename).apply(pd.Series)\n",
    "cat = pd.concat([cat, parsed], axis=1)\n",
    "\n",
    "# ---- Build a stable model_chain_id (prefer inventory fields; fall back to filename parse) ----\n",
    "def coalesce(a, b):\n",
    "    return a if (a is not None and str(a).strip() != \"\" and str(a).lower() != \"nan\") else b\n",
    "\n",
    "cat[\"gcm\"] = cat.apply(lambda r: coalesce(r.get(\"driving_model\"), r.get(\"gcm_fn\")), axis=1)\n",
    "cat[\"rcm\"] = cat.apply(lambda r: coalesce(r.get(\"rcm_model\"), r.get(\"rcm_fn\")), axis=1)\n",
    "cat[\"exp\"] = cat.apply(lambda r: coalesce(r.get(\"experiment\"), r.get(\"exp_fn\")), axis=1)\n",
    "cat[\"member\"] = cat.apply(lambda r: coalesce(r.get(\"ensemble_member\"), r.get(\"member_fn\")), axis=1)\n",
    "cat[\"var\"] = cat.apply(lambda r: coalesce(r.get(\"var_fn\"), None), axis=1)\n",
    "cat[\"domain\"] = cat.apply(lambda r: coalesce(r.get(\"domain\"), r.get(\"domain_fn\")), axis=1)\n",
    "\n",
    "cat[\"model_chain_id\"] = (\n",
    "    cat[\"gcm\"].fillna(\"NA\") + \"__\" +\n",
    "    cat[\"rcm\"].fillna(\"NA\") + \"__\" +\n",
    "    cat[\"exp\"].fillna(\"NA\") + \"__\" +\n",
    "    cat[\"member\"].fillna(\"NA\")\n",
    ")\n",
    "\n",
    "# ---- Basic filtering according to user inputs (still lightweight) ----\n",
    "def match_any(val: str, patterns) -> bool:\n",
    "    if patterns is None:\n",
    "        return True\n",
    "    s = str(val)\n",
    "    return any(p.lower() in s.lower() for p in patterns)\n",
    "\n",
    "# Filter by variable if we can\n",
    "if \"vars\" in cat.columns and VAR_NAME:\n",
    "    # keep if the dataset contains the variable, OR if filename indicates it\n",
    "    cat = cat[\n",
    "        cat[\"vars\"].fillna(\"\").str.contains(VAR_NAME, case=False, regex=False)\n",
    "        | cat[\"file_name\"].str.startswith(VAR_NAME + \"_\")\n",
    "    ].copy()\n",
    "\n",
    "# Filter by model/experiment patterns (against model_chain_id + file_name)\n",
    "if MODEL_FILTERS is not None:\n",
    "    cat = cat[cat.apply(lambda r: match_any(r[\"model_chain_id\"], MODEL_FILTERS) or match_any(r[\"file_name\"], MODEL_FILTERS), axis=1)].copy()\n",
    "\n",
    "if EXPERIMENT_FILTERS is not None:\n",
    "    cat = cat[cat.apply(lambda r: match_any(r[\"exp\"], EXPERIMENT_FILTERS) or match_any(r[\"file_name\"], EXPERIMENT_FILTERS), axis=1)].copy()\n",
    "\n",
    "# ---- Show result (catalog) ----\n",
    "show_cols = [\n",
    "    \"file_name\", \"var\", \"domain\", \"gcm\", \"rcm\", \"exp\", \"member\",\n",
    "    \"start_ym\", \"end_ym\", \"n_time\", \"time_start\", \"time_end\", \"model_chain_id\", \"path\"\n",
    "]\n",
    "show_cols = [c for c in show_cols if c in cat.columns]\n",
    "\n",
    "print(f\"Catalog rows after filtering: {len(cat)}\")\n",
    "display(cat[show_cols].sort_values([\"model_chain_id\", \"start_ym\", \"file_name\"]).reset_index(drop=True))\n",
    "\n",
    "# Keep catalog available for later cells\n",
    "catalog = cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a62a6f-8d06-42e3-9bf6-52bf3d48b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATIONS — annual mean + period mean (bulk), saved as derived NetCDFs\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "assert \"catalog\" in globals(), \"catalog not found. Run the catalog cell first.\"\n",
    "\n",
    "DERIVED_ROOT = EUROCORDEX_TEST_OUTPUT_ROOT / \"derived\"\n",
    "DERIVED_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FORCE_RECALC = False  # set True to overwrite derived files\n",
    "\n",
    "def safe_slug(s: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in str(s))\n",
    "\n",
    "def open_var(path: str | Path, var_name: str) -> xr.DataArray:\n",
    "    \"\"\"Open NetCDF and return selected variable as a loaded DataArray (safe to close file).\"\"\"\n",
    "    path = Path(path)\n",
    "    with xr.open_dataset(path, decode_times=True) as ds:\n",
    "        if var_name in ds:\n",
    "            da = ds[var_name]\n",
    "        else:\n",
    "            da = ds[list(ds.data_vars)[0]]  # fallback\n",
    "        da = da.squeeze().load()  # load so we can close the file safely\n",
    "    da.name = var_name\n",
    "    return da\n",
    "\n",
    "def annual_mean(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Monthly → annual mean (calendar-year), robust for cftime calendars.\"\"\"\n",
    "    if \"time\" not in da.dims:\n",
    "        raise ValueError(\"No 'time' dimension found.\")\n",
    "    return da.groupby(da[\"time\"].dt.year).mean(\"time\", skipna=True)\n",
    "\n",
    "def period_mean(da: xr.DataArray, y1: int, y2: int) -> xr.DataArray:\n",
    "    \"\"\"Mean over years y1..y2 inclusive, robust for cftime calendars.\"\"\"\n",
    "    if \"time\" not in da.dims:\n",
    "        raise ValueError(\"No 'time' dimension found.\")\n",
    "    years = da[\"time\"].dt.year\n",
    "    da_sel = da.sel(time=(years >= y1) & (years <= y2))\n",
    "    return da_sel.mean(\"time\", skipna=True)\n",
    "\n",
    "def write_ds(da: xr.DataArray, out_path: Path):\n",
    "    \"\"\"Write DataArray as a single-variable Dataset NetCDF.\"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    da.to_dataset(name=da.name).to_netcdf(out_path)\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in catalog.iterrows():\n",
    "    fp = Path(row[\"path\"])\n",
    "    if not fp.exists():\n",
    "        print(\"Skip missing:\", fp)\n",
    "        continue\n",
    "\n",
    "    model_id = safe_slug(row.get(\"model_chain_id\", \"NA\"))\n",
    "    start_ym = row.get(\"start_ym\", \"NA\")\n",
    "    end_ym = row.get(\"end_ym\", \"NA\")\n",
    "\n",
    "    da = open_var(fp, VAR_NAME)\n",
    "\n",
    "    # --- Annual mean ---\n",
    "    if DO_ANNUAL_MEAN:\n",
    "        outdir = DERIVED_ROOT / \"annual_mean\" / model_id\n",
    "        out_path = outdir / f\"{VAR_NAME}_annualmean_{start_ym}-{end_ym}.nc\"\n",
    "\n",
    "        if FORCE_RECALC or not out_path.exists():\n",
    "            ann = annual_mean(da)\n",
    "            write_ds(ann, out_path)\n",
    "\n",
    "        results.append({\n",
    "            \"product\": \"annual_mean\",\n",
    "            \"model_chain_id\": row.get(\"model_chain_id\"),\n",
    "            \"in_file\": fp.name,\n",
    "            \"out_file\": out_path.name,\n",
    "            \"out_path\": str(out_path),\n",
    "        })\n",
    "\n",
    "    # --- Period mean(s) ---\n",
    "    if DO_PERIOD_MEAN:\n",
    "        outdir = DERIVED_ROOT / \"period_mean\" / model_id\n",
    "\n",
    "        for (y1, y2) in PERIODS:\n",
    "            out_path = outdir / f\"{VAR_NAME}_periodmean_{y1}-{y2}_{start_ym}-{end_ym}.nc\"\n",
    "\n",
    "            if FORCE_RECALC or not out_path.exists():\n",
    "                pm = period_mean(da, y1, y2)\n",
    "                write_ds(pm, out_path)\n",
    "\n",
    "            results.append({\n",
    "                \"product\": f\"period_mean_{y1}-{y2}\",\n",
    "                \"model_chain_id\": row.get(\"model_chain_id\"),\n",
    "                \"in_file\": fp.name,\n",
    "                \"out_file\": out_path.name,\n",
    "                \"out_path\": str(out_path),\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "print(\"Derived root:\", DERIVED_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812183f-3141-487e-8e49-ba13e1862351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TABLE OUTPUT — run CSA aggregation for all derived products and write CSV\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import warnings\n",
    "\n",
    "# requires: CSA_LIST, results_df, VAR_NAME, load_fua_boundary, EUROCORDEX_TEST_OUTPUT_ROOT\n",
    "\n",
    "TABLES_DIR = EUROCORDEX_TEST_OUTPUT_ROOT / \"tables\"\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_lon_lat_from_da(da: xr.DataArray):\n",
    "    for lon, lat in [(\"lon\",\"lat\"), (\"longitude\",\"latitude\"), (\"LON\",\"LAT\"), (\"nav_lon\",\"nav_lat\")]:\n",
    "        if lon in da.coords and lat in da.coords:\n",
    "            return da[lon].values, da[lat].values\n",
    "    return None, None\n",
    "\n",
    "def mask_from_polygon(lon, lat, polygon):\n",
    "    from shapely import vectorized\n",
    "    if lon.ndim == 1 and lat.ndim == 1:\n",
    "        xx, yy = np.meshgrid(lon, lat)\n",
    "        return vectorized.contains(polygon, xx, yy)\n",
    "    return vectorized.contains(polygon, lon, lat)\n",
    "\n",
    "def csa_means_from_derived(out_path: Path, model_chain_id: str, product: str) -> list[dict]:\n",
    "    out_path = Path(out_path)\n",
    "    rows = []\n",
    "\n",
    "    ds = xr.open_dataset(out_path, decode_times=True)\n",
    "    try:\n",
    "        da = ds[VAR_NAME].squeeze()\n",
    "\n",
    "        lon, lat = get_lon_lat_from_da(da if \"year\" not in da.dims else da.isel(year=0))\n",
    "        if lon is None or lat is None:\n",
    "            warnings.warn(f\"Skip (no lon/lat coords): {out_path.name}\")\n",
    "            return rows\n",
    "\n",
    "        # iterate annual years if present; else single map\n",
    "        if \"year\" in da.dims:\n",
    "            year_vals = [int(y) for y in da[\"year\"].values]\n",
    "            slices = [(y, da.sel(year=y)) for y in year_vals]\n",
    "        else:\n",
    "            slices = [(None, da)]\n",
    "\n",
    "        for year_val, da2 in slices:\n",
    "            arr = np.asarray(da2.values, dtype=\"float64\")\n",
    "\n",
    "            for csa in CSA_LIST:\n",
    "                gdf = load_fua_boundary(csa)\n",
    "                if gdf is None or gdf.empty:\n",
    "                    warnings.warn(f\"No FUA geometry for CSA={csa}\")\n",
    "                    continue\n",
    "\n",
    "                bg = gdf\n",
    "                if getattr(bg, \"crs\", None) is not None:\n",
    "                    bg = bg.to_crs(\"EPSG:4326\")\n",
    "\n",
    "                # dissolve to one polygon (GeoPandas >= 0.14)\n",
    "                poly = bg.geometry.union_all()\n",
    "\n",
    "                m = mask_from_polygon(lon, lat, poly)\n",
    "                if m.shape != arr.shape:\n",
    "                    warnings.warn(f\"Skip CSA={csa} (mask shape {m.shape} != data {arr.shape}) for {out_path.name}\")\n",
    "                    continue\n",
    "\n",
    "                n_cells = int(np.sum(m))\n",
    "                mean_val = float(np.nanmean(arr[m])) if n_cells > 0 else np.nan\n",
    "\n",
    "                rows.append({\n",
    "                    \"csa\": csa,\n",
    "                    \"model_chain_id\": model_chain_id,\n",
    "                    \"product\": product,\n",
    "                    \"year\": year_val,\n",
    "                    \"mean\": mean_val,\n",
    "                    \"n_cells\": n_cells,\n",
    "                    \"derived_file\": out_path.name,\n",
    "                })\n",
    "\n",
    "    finally:\n",
    "        ds.close()\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---- run across ALL derived outputs (annual + period) ----\n",
    "all_rows = []\n",
    "for _, r in results_df.iterrows():\n",
    "    out_path = Path(r[\"out_path\"])\n",
    "    if not out_path.exists():\n",
    "        warnings.warn(f\"Missing derived output: {out_path}\")\n",
    "        continue\n",
    "\n",
    "    all_rows.extend(\n",
    "        csa_means_from_derived(\n",
    "            out_path=out_path,\n",
    "            model_chain_id=r.get(\"model_chain_id\"),\n",
    "            product=r.get(\"product\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "tables_df = pd.DataFrame(all_rows)\n",
    "\n",
    "# tidy: add a period column (only for period means)\n",
    "tables_df[\"period\"] = None\n",
    "mask_period = tables_df[\"product\"].astype(str).str.startswith(\"period_mean_\")\n",
    "tables_df.loc[mask_period, \"period\"] = tables_df.loc[mask_period, \"product\"].str.replace(\"period_mean_\", \"\", regex=False)\n",
    "# annual: year is set, period stays None\n",
    "\n",
    "display(tables_df.head(20))\n",
    "print(\"Rows:\", len(tables_df))\n",
    "\n",
    "# ---- write outputs ----\n",
    "csv_path = TABLES_DIR / f\"CSA_FUA_means_{VAR_NAME}.csv\"\n",
    "tables_df.to_csv(csv_path, index=False)\n",
    "print(\"Wrote:\", csv_path)\n",
    "\n",
    "# Optional: a wide pivot for quick inspection (one table per product)\n",
    "wide = tables_df.pivot_table(index=[\"csa\"], columns=[\"product\", \"model_chain_id\", \"year\"], values=\"mean\")\n",
    "display(wide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604ee9c-b406-41a2-94e5-01ff7345fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PANEL PLOT — 7 CSAs (FUA zoom), one product + one model chain\n",
    "# =============================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# ---- USER CHOICE FOR THE PANEL ----\n",
    "PANEL_PRODUCT = \"period_mean_2031-2040\"   # e.g. \"period_mean_2031-2040\" or \"annual_mean\"\n",
    "PANEL_MODEL_FILTER = None                # substring to pick one model chain, e.g. \"NCC-NorESM1-M\"\n",
    "CMAP = \"viridis\"                         # keep simple for now\n",
    "ROBUST = True                            # use percentiles to avoid outliers dominating\n",
    "PCTL = (2, 98)                           # robust scaling percentiles\n",
    "ADD_BOUNDARY = True                      # overlay FUA boundary\n",
    "DPI = 200\n",
    "\n",
    "assert \"results_df\" in globals() and not results_df.empty, \"results_df missing (run calculations).\"\n",
    "\n",
    "# --- helper: get lon/lat coord names and arrays ---\n",
    "def get_lon_lat(da: xr.DataArray):\n",
    "    for lon, lat in [(\"lon\",\"lat\"), (\"longitude\",\"latitude\"), (\"LON\",\"LAT\"), (\"nav_lon\",\"nav_lat\")]:\n",
    "        if lon in da.coords and lat in da.coords:\n",
    "            return da[lon].values, da[lat].values\n",
    "    return None, None\n",
    "\n",
    "def dissolve_to_lonlat_polygon(csa: str):\n",
    "    gdf = load_fua_boundary(csa)\n",
    "    if gdf is None or gdf.empty:\n",
    "        return None\n",
    "    bg = gdf\n",
    "    if getattr(bg, \"crs\", None) is not None:\n",
    "        bg = bg.to_crs(\"EPSG:4326\")\n",
    "    return bg.geometry.union_all()\n",
    "\n",
    "# --- pick one derived file matching product (+ optional model filter) ---\n",
    "sub = results_df[results_df[\"product\"] == PANEL_PRODUCT].copy()\n",
    "if sub.empty:\n",
    "    raise ValueError(f\"No derived outputs for PANEL_PRODUCT='{PANEL_PRODUCT}'. \"\n",
    "                     f\"Available: {sorted(results_df['product'].unique())}\")\n",
    "\n",
    "if PANEL_MODEL_FILTER is not None:\n",
    "    sub = sub[sub[\"model_chain_id\"].astype(str).str.contains(PANEL_MODEL_FILTER, case=False, na=False)].copy()\n",
    "    if sub.empty:\n",
    "        raise ValueError(f\"No derived outputs match PANEL_MODEL_FILTER='{PANEL_MODEL_FILTER}'.\")\n",
    "\n",
    "# choose the first match (you can change this logic later)\n",
    "picked = sub.iloc[0]\n",
    "DERIVED_PATH = Path(picked[\"out_path\"])\n",
    "MODEL_ID = picked.get(\"model_chain_id\")\n",
    "\n",
    "print(\"Using derived file:\", DERIVED_PATH.name)\n",
    "print(\"Model chain:\", MODEL_ID)\n",
    "\n",
    "# --- load map (annual → pick first year for now) ---\n",
    "ds = xr.open_dataset(DERIVED_PATH, decode_times=True)\n",
    "try:\n",
    "    da = ds[VAR_NAME].squeeze()\n",
    "    year_val = None\n",
    "    if \"year\" in da.dims:\n",
    "        year_val = int(da[\"year\"].values[0])\n",
    "        da = da.isel(year=0)\n",
    "\n",
    "    lon, lat = get_lon_lat(da)\n",
    "    if lon is None or lat is None:\n",
    "        raise RuntimeError(\"No lon/lat coordinates found in derived product. Needed for map plotting/zoom.\")\n",
    "\n",
    "    # --- compute shared color scale across all CSAs (based on each CSA-masked pixels) ---\n",
    "    all_vals = []\n",
    "    csa_polys = {}\n",
    "\n",
    "    for csa in CSA_LIST:\n",
    "        poly = dissolve_to_lonlat_polygon(csa)\n",
    "        if poly is None:\n",
    "            warnings.warn(f\"Skip CSA={csa} (no polygon)\")\n",
    "            continue\n",
    "        csa_polys[csa] = poly\n",
    "\n",
    "        # mask values within polygon using shapely.vectorized\n",
    "        from shapely import vectorized\n",
    "        if lon.ndim == 1 and lat.ndim == 1:\n",
    "            xx, yy = np.meshgrid(lon, lat)\n",
    "            m = vectorized.contains(poly, xx, yy)\n",
    "        else:\n",
    "            m = vectorized.contains(poly, lon, lat)\n",
    "\n",
    "        arr = np.asarray(da.values, dtype=\"float64\")\n",
    "        if m.shape == arr.shape and np.any(m):\n",
    "            all_vals.append(arr[m])\n",
    "\n",
    "    if not all_vals:\n",
    "        raise RuntimeError(\"No CSA masks produced any values. Check polygons overlap lon/lat grid.\")\n",
    "\n",
    "    all_vals = np.concatenate(all_vals)\n",
    "    all_vals = all_vals[np.isfinite(all_vals)]\n",
    "\n",
    "    if ROBUST and all_vals.size > 0:\n",
    "        vmin, vmax = np.percentile(all_vals, PCTL)\n",
    "    else:\n",
    "        vmin, vmax = float(np.nanmin(all_vals)), float(np.nanmax(all_vals))\n",
    "\n",
    "    print(\"Color scale:\", vmin, \"to\", vmax)\n",
    "\n",
    "    # --- plot panel (7 CSAs) ---\n",
    "    n = len(CSA_LIST)\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4*ncols, 3.5*nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    # store one im for shared colorbar\n",
    "    im_last = None\n",
    "\n",
    "    for i, csa in enumerate(CSA_LIST):\n",
    "        ax = axes[i]\n",
    "        poly = csa_polys.get(csa, None)\n",
    "        if poly is None:\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(f\"{csa} (no boundary)\")\n",
    "            continue\n",
    "\n",
    "        arr = np.asarray(da.values, dtype=\"float64\")\n",
    "\n",
    "        # draw full map, then zoom to CSA bounds (simple + reliable)\n",
    "        if lon.ndim == 1 and lat.ndim == 1:\n",
    "            extent = [float(np.nanmin(lon)), float(np.nanmax(lon)),\n",
    "                      float(np.nanmin(lat)), float(np.nanmax(lat))]\n",
    "            im = ax.imshow(arr, origin=\"lower\", extent=extent, cmap=CMAP, vmin=vmin, vmax=vmax)\n",
    "        else:\n",
    "            im = ax.pcolormesh(lon, lat, arr, shading=\"auto\", cmap=CMAP, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        im_last = im\n",
    "\n",
    "        # overlay boundary + zoom\n",
    "        if ADD_BOUNDARY:\n",
    "            try:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax.plot(x, y, linewidth=1.2, color=\"black\")\n",
    "            except Exception:\n",
    "                # multipolygons\n",
    "                try:\n",
    "                    for geom in getattr(poly, \"geoms\", []):\n",
    "                        x, y = geom.exterior.xy\n",
    "                        ax.plot(x, y, linewidth=1.2, color=\"black\")\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Boundary plot failed for {csa}: {e}\")\n",
    "\n",
    "        minx, miny, maxx, maxy = poly.bounds\n",
    "        pad_x = (maxx - minx) * 0.15\n",
    "        pad_y = (maxy - miny) * 0.15\n",
    "        ax.set_xlim(minx - pad_x, maxx + pad_x)\n",
    "        ax.set_ylim(miny - pad_y, maxy + pad_y)\n",
    "\n",
    "        ax.set_title(csa)\n",
    "        ax.set_xlabel(\"lon\")\n",
    "        ax.set_ylabel(\"lat\")\n",
    "\n",
    "    # turn off any unused axes\n",
    "    for j in range(len(CSA_LIST), len(axes)):\n",
    "        axes[j].set_axis_off()\n",
    "\n",
    "    # shared colorbar\n",
    "    # --- shared colorbar in dedicated axis ---\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(im_last, cax=cax)\n",
    "\n",
    "    units = str(da.attrs.get(\"units\", \"\")).strip()\n",
    "    if units:\n",
    "        cbar.set_label(units)\n",
    "\n",
    "\n",
    "    sup = f\"{VAR_NAME} — {PANEL_PRODUCT}\"\n",
    "    if year_val is not None:\n",
    "        sup += f\" (year={year_val})\"\n",
    "    sup += f\"\\n{MODEL_ID}\"\n",
    "    fig.suptitle(sup, y=0.995)\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    out_png = EUROCORDEX_TEST_OUTPUT_ROOT / \"plots\" / \"panel\" / f\"{VAR_NAME}_{PANEL_PRODUCT}_panel_{safe_slug(str(MODEL_ID))}.png\"\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=DPI, bbox_inches=\"tight\")\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    print(\"Saved panel:\", out_png)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "finally:\n",
    "    ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46796f-70ca-4e74-bb4a-7af33213cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SANITY PLOT — quick look at one derived product\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "assert \"results_df\" in globals() and not results_df.empty, \"No derived outputs found.\"\n",
    "\n",
    "# Pick first annual mean output by default (edit if you want)\n",
    "pick = results_df[results_df[\"product\"] == \"annual_mean\"].head(1)\n",
    "if pick.empty:\n",
    "    pick = results_df.head(1)\n",
    "\n",
    "out_path = Path(pick.iloc[0][\"out_path\"])\n",
    "print(\"Plotting:\", out_path)\n",
    "\n",
    "ds = xr.open_dataset(out_path)\n",
    "da = ds[VAR_NAME]\n",
    "\n",
    "# If annual_mean: pick first year slice to plot\n",
    "if \"year\" in da.dims:\n",
    "    da2 = da.isel(year=0)\n",
    "    title = f\"{VAR_NAME} annual mean — year={int(da['year'].values[0])}\"\n",
    "else:\n",
    "    da2 = da\n",
    "    title = f\"{VAR_NAME} period mean\"\n",
    "\n",
    "arr = np.asarray(da2.values, dtype=\"float64\")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(arr, origin=\"lower\")\n",
    "plt.title(title)\n",
    "plt.colorbar(label=str(da2.attrs.get(\"units\", \"\")))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057c3c3-5267-4563-86ae-ee6ed8f0db64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
